{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc_mean = nn.Linear(hidden_size, latent_size)\n",
    "        self.fc_logvar = nn.Linear(hidden_size, latent_size)\n",
    "        self.fc2 = nn.Linear(latent_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc_mean(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc2(z))\n",
    "        return self.fc3(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit():\n",
    "    \n",
    "    model = VAE()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    epoch_list = []\n",
    "    loss_list = []\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for epoch in tqdm_notebook(range(num_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data_tensor)\n",
    "        BCE = F.mse_loss(recon_batch, data_tensor, reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        loss = BCE + KLD\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_list.append(epoch+1)\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        pd.DataFrame(np.array([epoch_list, loss_list]).T, columns = ['epoch', 'loss'])\\\n",
    "        .to_csv(f'epoch_{num_epochs}_hidden_size_{hidden_size}_latent_size_{latent_size}.tsv', index = 0, sep = '\\t')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(epoch_list, loss_list)\n",
    "        plt.yscale('log')\n",
    "        plt.show()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, BCE: {BCE.item():.4f}, KLD: {KLD.item():.4f}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model, data_tensor):\n",
    "    with torch.no_grad():\n",
    "        latent_representation = model.encode(data_tensor)[0].numpy()\n",
    "        \n",
    "    return pd.DataFrame(latent_representation, columns=[f'latent_{i}' for i in range(latent_size)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
